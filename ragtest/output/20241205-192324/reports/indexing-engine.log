19:23:24,887 graphrag.config.read_dotenv INFO Loading pipeline .env file
19:23:24,892 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 51",
        "type": "openai_chat",
        "model": "glm-4",
        "max_tokens": 2000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:3000/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "./",
    "reporting": {
        "type": "file",
        "base_dir": "inputs/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "inputs/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_embedding",
            "model": "embedding-2",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "glm-4",
            "max_tokens": 2000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:3000/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
19:23:24,898 graphrag.index.create_pipeline_config INFO skipping workflows 
19:23:24,900 graphrag.index.run INFO Running pipeline
19:23:24,900 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at inputs\artifacts
19:23:24,902 graphrag.index.input.load_input INFO loading input from root_dir=input
19:23:24,902 graphrag.index.input.load_input INFO using file storage for input
19:23:24,903 graphrag.index.storage.file_pipeline_storage INFO search input for files matching .*\.txt$
19:23:24,904 graphrag.index.input.text INFO found text files from input, found [('1.txt', {}), ('2.txt', {}), ('3.txt', {}), ('4.txt', {}), ('5.txt', {}), ('6.txt', {}), ('7.txt', {}), ('8.txt', {}), ('9.txt', {})]
19:23:24,913 graphrag.index.input.text INFO Found 9 files, loading 9
19:23:24,914 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_final_covariates', 'create_summarized_entities', 'join_text_units_to_covariate_ids', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
19:23:24,914 graphrag.index.run INFO Final # of rows loaded: 9
19:23:25,107 graphrag.index.run INFO Running workflow: create_base_text_units...
19:23:25,109 graphrag.index.run INFO dependencies for create_base_text_units: []
19:23:25,112 datashaper.workflow.workflow INFO executing verb orderby
19:23:25,117 datashaper.workflow.workflow INFO executing verb zip
19:23:25,121 datashaper.workflow.workflow INFO executing verb aggregate_override
19:23:25,129 datashaper.workflow.workflow INFO executing verb chunk
19:23:25,275 datashaper.workflow.workflow INFO executing verb select
19:23:25,282 datashaper.workflow.workflow INFO executing verb unroll
19:23:25,288 datashaper.workflow.workflow INFO executing verb rename
19:23:25,291 datashaper.workflow.workflow INFO executing verb genid
19:23:25,295 datashaper.workflow.workflow INFO executing verb unzip
19:23:25,298 datashaper.workflow.workflow INFO executing verb copy
19:23:25,301 datashaper.workflow.workflow INFO executing verb filter
19:23:25,312 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:23:25,533 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
19:23:25,533 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
19:23:25,534 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
19:23:25,563 datashaper.workflow.workflow INFO executing verb entity_extract
19:23:25,570 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:3000/v1
19:23:25,771 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for glm-4: TPM=0, RPM=0
19:23:25,773 graphrag.index.llm.load_llm INFO create concurrency limiter for glm-4: 25
19:23:31,506 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:31,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.655999999999949. input_tokens=1734, output_tokens=208
19:23:37,78 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:37,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.25. input_tokens=1805, output_tokens=252
19:23:38,968 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:38,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.140000000000327. input_tokens=2899, output_tokens=480
19:23:41,483 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:41,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.625. input_tokens=2834, output_tokens=652
19:23:41,676 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:41,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.875. input_tokens=2027, output_tokens=558
19:23:43,231 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:43,232 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.359999999999673. input_tokens=2900, output_tokens=560
19:23:43,701 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:43,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.875. input_tokens=2900, output_tokens=778
19:23:44,883 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:44,885 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.07799999999952. input_tokens=2900, output_tokens=784
19:23:44,900 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:44,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.063000000000102. input_tokens=1947, output_tokens=621
19:23:45,40 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:45,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.23399999999947. input_tokens=2900, output_tokens=803
19:23:45,181 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:45,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.32800000000043. input_tokens=2900, output_tokens=800
19:23:46,169 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:46,170 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.07799999999952. input_tokens=1786, output_tokens=385
19:23:46,423 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:46,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.59400000000005. input_tokens=2901, output_tokens=884
19:23:47,608 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:47,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.79699999999957. input_tokens=2900, output_tokens=870
19:23:48,685 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:48,686 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.82800000000043. input_tokens=2900, output_tokens=873
19:23:49,841 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:49,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.015000000000327. input_tokens=2900, output_tokens=945
19:23:50,309 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:50,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.42200000000048. input_tokens=34, output_tokens=177
19:23:50,559 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:50,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.71900000000005. input_tokens=2899, output_tokens=1025
19:23:50,569 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:50,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.76599999999962. input_tokens=2901, output_tokens=845
19:23:52,850 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:52,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.01599999999962. input_tokens=2455, output_tokens=904
19:23:52,977 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:52,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.188000000000102. input_tokens=2900, output_tokens=991
19:23:53,725 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:53,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.875. input_tokens=2899, output_tokens=1043
19:23:54,956 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:54,957 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.125. input_tokens=2900, output_tokens=998
19:23:55,520 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:55,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.70300000000043. input_tokens=2900, output_tokens=1007
19:23:56,466 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:56,466 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.561999999999898. input_tokens=34, output_tokens=547
19:23:56,913 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:56,913 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:56,915 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.07799999999952. input_tokens=2900, output_tokens=1305
19:23:56,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.092999999999847. input_tokens=2139, output_tokens=865
19:23:57,360 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:57,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.186999999999898. input_tokens=34, output_tokens=466
19:23:57,498 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:57,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.640000000000327. input_tokens=2079, output_tokens=754
19:23:58,573 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:58,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.407000000000153. input_tokens=34, output_tokens=519
19:23:58,877 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:23:58,878 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.375. input_tokens=2886, output_tokens=1119
19:24:00,15 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:00,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.594000000000051. input_tokens=34, output_tokens=483
19:24:01,797 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:01,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.95299999999952. input_tokens=34, output_tokens=483
19:24:03,78 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:03,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.032000000000153. input_tokens=34, output_tokens=641
19:24:03,206 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:03,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.234000000000378. input_tokens=34, output_tokens=359
19:24:03,803 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:03,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.125. input_tokens=2900, output_tokens=858
19:24:04,922 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:04,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.20300000000043. input_tokens=34, output_tokens=363
19:24:05,47 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:05,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.35900000000038. input_tokens=2014, output_tokens=904
19:24:05,119 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:05,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.515000000000327. input_tokens=34, output_tokens=759
19:24:06,580 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:06,581 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.90599999999995. input_tokens=34, output_tokens=747
19:24:07,966 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:07,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.0. input_tokens=34, output_tokens=461
19:24:09,495 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:09,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.57800000000043. input_tokens=34, output_tokens=483
19:24:09,609 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:09,611 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.04699999999957. input_tokens=34, output_tokens=738
19:24:10,86 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:10,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.85900000000038. input_tokens=2901, output_tokens=1071
19:24:12,543 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:12,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.967999999999847. input_tokens=34, output_tokens=479
19:24:12,666 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:12,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.70299999999952. input_tokens=2900, output_tokens=1235
19:24:12,677 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:12,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.375. input_tokens=34, output_tokens=800
19:24:12,704 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:12,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.219000000000051. input_tokens=34, output_tokens=565
19:24:12,952 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:12,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.09400000000005. input_tokens=34, output_tokens=799
19:24:14,518 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:14,518 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:14,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.063000000000102. input_tokens=34, output_tokens=640
19:24:14,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.719000000000051. input_tokens=34, output_tokens=558
19:24:14,556 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:14,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.03099999999995. input_tokens=34, output_tokens=752
19:24:16,204 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:16,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.186999999999898. input_tokens=34, output_tokens=679
19:24:17,750 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:17,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.28099999999995. input_tokens=2900, output_tokens=1413
19:24:18,320 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:18,321 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.515999999999622. input_tokens=34, output_tokens=580
19:24:19,16 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:19,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.657000000000153. input_tokens=34, output_tokens=745
19:24:19,217 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:19,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.640000000000327. input_tokens=34, output_tokens=1252
19:24:19,707 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:19,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.79700000000048. input_tokens=34, output_tokens=841
19:24:20,145 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:20,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.094000000000051. input_tokens=34, output_tokens=675
19:24:27,735 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:27,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.65599999999995. input_tokens=34, output_tokens=785
19:24:31,70 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:31,71 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.407000000000153. input_tokens=34, output_tokens=776
19:24:32,841 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:32,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.07800000000043. input_tokens=34, output_tokens=559
19:24:42,66 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:42,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.0. input_tokens=34, output_tokens=1440
19:24:50,60 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:50,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.17200000000048. input_tokens=34, output_tokens=2357
19:24:50,75 datashaper.workflow.workflow INFO executing verb merge_graphs
19:24:50,109 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
19:24:50,320 graphrag.index.run INFO Running workflow: create_final_covariates...
19:24:50,320 graphrag.index.run INFO dependencies for create_final_covariates: ['create_base_text_units']
19:24:50,321 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
19:24:50,333 datashaper.workflow.workflow INFO executing verb extract_covariates
19:24:54,889 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:24:54,890 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.5. input_tokens=2314, output_tokens=55
19:25:01,791 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:01,793 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.375. input_tokens=1150, output_tokens=292
19:25:03,271 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:03,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.891000000000531. input_tokens=1555, output_tokens=236
19:25:06,365 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:06,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.967999999999847. input_tokens=2313, output_tokens=498
19:25:06,610 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:06,611 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.828000000000429. input_tokens=1202, output_tokens=84
19:25:07,243 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:07,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.875. input_tokens=2315, output_tokens=408
19:25:08,809 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:08,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.40599999999995. input_tokens=2314, output_tokens=510
19:25:09,519 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:09,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.14100000000053. input_tokens=2314, output_tokens=454
19:25:10,569 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:10,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.188000000000102. input_tokens=2314, output_tokens=508
19:25:11,653 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:11,655 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.235000000000582. input_tokens=2314, output_tokens=439
19:25:12,888 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:12,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.51599999999962. input_tokens=1443, output_tokens=830
19:25:13,60 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:13,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.64100000000053. input_tokens=2250, output_tokens=671
19:25:13,94 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:13,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.734999999999673. input_tokens=2314, output_tokens=593
19:25:15,544 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:15,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.139999999999418. input_tokens=1363, output_tokens=781
19:25:17,35 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:17,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.639999999999418. input_tokens=1871, output_tokens=857
19:25:17,688 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:17,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.26599999999962. input_tokens=2314, output_tokens=676
19:25:17,717 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:17,718 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.29700000000048. input_tokens=2313, output_tokens=686
19:25:17,849 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:17,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.5. input_tokens=2314, output_tokens=905
19:25:19,240 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:19,242 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.85900000000038. input_tokens=2313, output_tokens=983
19:25:19,480 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:19,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.109999999999673. input_tokens=2314, output_tokens=797
19:25:19,566 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:19,568 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.17199999999957. input_tokens=2315, output_tokens=660
19:25:21,192 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:21,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.782000000000153. input_tokens=1495, output_tokens=774
19:25:21,792 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:21,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.40599999999995. input_tokens=1221, output_tokens=649
19:25:23,305 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:23,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.42200000000048. input_tokens=2302, output_tokens=903
19:25:27,54 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:27,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.64100000000053. input_tokens=2314, output_tokens=972
19:25:29,745 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:29,747 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.375. input_tokens=2314, output_tokens=1274
19:25:29,818 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:29,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.453999999999724. input_tokens=2314, output_tokens=570
19:25:29,987 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:29,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.717999999999847. input_tokens=2314, output_tokens=1056
19:25:32,94 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:32,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.04699999999957. input_tokens=19, output_tokens=570
19:25:32,262 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:32,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.686999999999898. input_tokens=19, output_tokens=628
19:25:32,343 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:32,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.79700000000048. input_tokens=19, output_tokens=460
19:25:32,616 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:32,617 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.73400000000038. input_tokens=19, output_tokens=671
19:25:34,246 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:34,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.140000000000327. input_tokens=19, output_tokens=542
19:25:34,465 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:34,466 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.21900000000005. input_tokens=2315, output_tokens=836
19:25:34,788 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:34,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.264999999999418. input_tokens=19, output_tokens=628
19:25:36,819 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:36,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.96900000000005. input_tokens=19, output_tokens=558
19:25:38,841 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:38,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.21900000000005. input_tokens=2314, output_tokens=1002
19:25:39,442 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:39,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.79699999999957. input_tokens=19, output_tokens=802
19:25:41,320 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:41,322 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.92199999999957. input_tokens=2314, output_tokens=1386
19:25:42,359 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:42,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.64099999999962. input_tokens=19, output_tokens=560
19:25:42,701 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:42,703 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.0. input_tokens=19, output_tokens=553
19:25:43,372 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:43,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.32800000000043. input_tokens=19, output_tokens=651
19:25:43,774 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:43,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.985000000000582. input_tokens=19, output_tokens=619
19:25:44,541 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:44,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.342999999999847. input_tokens=19, output_tokens=623
19:25:48,468 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:48,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.21900000000005. input_tokens=19, output_tokens=870
19:25:49,801 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:49,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.0. input_tokens=1429, output_tokens=1064
19:25:50,993 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:50,994 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.171000000000276. input_tokens=19, output_tokens=733
19:25:52,943 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:52,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.203999999999724. input_tokens=19, output_tokens=699
19:25:53,47 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:53,48 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.04699999999957. input_tokens=19, output_tokens=641
19:25:53,107 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:53,109 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.859999999999673. input_tokens=19, output_tokens=620
19:25:54,332 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:54,334 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.85900000000038. input_tokens=19, output_tokens=935
19:25:57,664 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:57,666 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.20299999999952. input_tokens=19, output_tokens=563
19:25:58,315 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:58,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.063000000000102. input_tokens=19, output_tokens=794
19:25:59,807 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:25:59,808 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.75. input_tokens=19, output_tokens=892
19:26:00,999 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:01,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.171000000000276. input_tokens=19, output_tokens=695
19:26:01,281 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:01,283 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.5. input_tokens=19, output_tokens=1103
19:26:02,349 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:02,350 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.25. input_tokens=19, output_tokens=1009
19:26:05,508 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:05,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.65599999999995. input_tokens=19, output_tokens=899
19:26:05,892 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:05,893 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.5630000000001. input_tokens=19, output_tokens=1070
19:26:06,364 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:06,365 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.75. input_tokens=19, output_tokens=994
19:26:07,425 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:07,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.85900000000038. input_tokens=19, output_tokens=1361
19:26:10,945 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:10,946 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.14099999999962. input_tokens=19, output_tokens=647
19:26:18,875 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:18,884 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 55.57799999999952. input_tokens=19, output_tokens=1273
19:26:34,568 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:34,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.25. input_tokens=19, output_tokens=2018
19:26:34,580 datashaper.workflow.workflow INFO executing verb window
19:26:34,585 datashaper.workflow.workflow INFO executing verb genid
19:26:34,590 datashaper.workflow.workflow INFO executing verb convert
19:26:34,600 datashaper.workflow.workflow INFO executing verb rename
19:26:34,605 datashaper.workflow.workflow INFO executing verb select
19:26:34,608 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
19:26:34,806 graphrag.index.run INFO Running workflow: create_summarized_entities...
19:26:34,806 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
19:26:34,807 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
19:26:34,823 datashaper.workflow.workflow INFO executing verb summarize_descriptions
19:26:37,267 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:37,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.391000000000531. input_tokens=295, output_tokens=110
19:26:37,372 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:37,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.436999999999898. input_tokens=310, output_tokens=118
19:26:37,779 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:37,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.860000000000582. input_tokens=329, output_tokens=131
19:26:37,827 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:37,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.907000000000153. input_tokens=295, output_tokens=133
19:26:38,253 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:38,253 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.344000000000051. input_tokens=345, output_tokens=151
19:26:38,265 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:38,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.375. input_tokens=279, output_tokens=137
19:26:38,421 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:38,421 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:38,422 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5. input_tokens=320, output_tokens=171
19:26:38,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.530999999999949. input_tokens=374, output_tokens=142
19:26:38,552 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:38,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.625. input_tokens=346, output_tokens=137
19:26:38,631 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:38,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.70299999999952. input_tokens=296, output_tokens=180
19:26:39,108 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:39,109 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:39,109 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.171999999999571. input_tokens=271, output_tokens=200
19:26:39,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.188000000000102. input_tokens=300, output_tokens=193
19:26:39,128 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:39,129 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.219000000000051. input_tokens=300, output_tokens=187
19:26:39,356 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:39,357 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.45299999999952. input_tokens=300, output_tokens=198
19:26:39,444 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:39,445 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.515999999999622. input_tokens=324, output_tokens=187
19:26:39,606 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:39,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.70299999999952. input_tokens=407, output_tokens=231
19:26:39,859 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:39,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.984000000000378. input_tokens=302, output_tokens=234
19:26:39,895 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:39,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.625. input_tokens=285, output_tokens=145
19:26:40,204 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:40,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.328000000000429. input_tokens=368, output_tokens=260
19:26:40,660 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:40,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.764999999999418. input_tokens=373, output_tokens=307
19:26:40,766 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:40,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0. input_tokens=282, output_tokens=125
19:26:40,916 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:40,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5469999999995707. input_tokens=358, output_tokens=197
19:26:40,953 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:40,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.032000000000153. input_tokens=502, output_tokens=295
19:26:40,976 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:40,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.344000000000051. input_tokens=268, output_tokens=114
19:26:41,360 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:41,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.453000000000429. input_tokens=672, output_tokens=324
19:26:41,478 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:41,479 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.344000000000051. input_tokens=282, output_tokens=105
19:26:41,554 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:41,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.29700000000048. input_tokens=295, output_tokens=123
19:26:41,578 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:41,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1409999999996217. input_tokens=290, output_tokens=164
19:26:41,591 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:41,592 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.3280000000004293. input_tokens=338, output_tokens=178
19:26:41,595 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:41,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.780999999999949. input_tokens=330, output_tokens=204
19:26:41,806 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:41,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.905999999999949. input_tokens=1088, output_tokens=373
19:26:42,22 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:42,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.469000000000051. input_tokens=301, output_tokens=177
19:26:42,211 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:42,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.328000000000429. input_tokens=792, output_tokens=385
19:26:42,287 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:42,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.936999999999898. input_tokens=284, output_tokens=147
19:26:42,436 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:42,436 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.0. input_tokens=297, output_tokens=168
19:26:42,539 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:42,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6719999999995707. input_tokens=323, output_tokens=154
19:26:42,619 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:42,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.717999999999847. input_tokens=276, output_tokens=141
19:26:42,799 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:42,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.905999999999949. input_tokens=885, output_tokens=404
19:26:43,27 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:43,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.42200000000048. input_tokens=367, output_tokens=173
19:26:43,234 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:43,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.780999999999949. input_tokens=347, output_tokens=194
19:26:43,355 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:43,356 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:43,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.234999999999673. input_tokens=297, output_tokens=213
19:26:43,357 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.234999999999673. input_tokens=362, output_tokens=222
19:26:43,601 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:43,602 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.3909999999996217. input_tokens=319, output_tokens=169
19:26:43,970 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:43,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0159999999996217. input_tokens=321, output_tokens=155
19:26:44,495 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:44,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2030000000004293. input_tokens=279, output_tokens=103
19:26:44,573 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:44,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0159999999996217. input_tokens=263, output_tokens=145
19:26:44,607 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:44,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.82799999999952. input_tokens=272, output_tokens=207
19:26:44,681 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:44,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0780000000004293. input_tokens=313, output_tokens=173
19:26:44,740 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:44,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.155999999999949. input_tokens=296, output_tokens=148
19:26:44,818 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:44,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2349999999996726. input_tokens=311, output_tokens=188
19:26:44,900 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:44,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.42200000000048. input_tokens=296, output_tokens=167
19:26:45,32 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:45,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.375. input_tokens=398, output_tokens=245
19:26:45,87 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:45,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.109000000000378. input_tokens=300, output_tokens=217
19:26:45,99 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:45,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4849999999996726. input_tokens=297, output_tokens=121
19:26:45,356 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:45,357 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9219999999995707. input_tokens=281, output_tokens=174
19:26:45,556 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:45,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.188000000000102. input_tokens=316, output_tokens=162
19:26:45,560 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:45,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.75. input_tokens=327, output_tokens=226
19:26:45,877 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:45,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.344000000000051. input_tokens=298, output_tokens=171
19:26:46,283 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:46,283 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.264999999999418. input_tokens=327, output_tokens=223
19:26:46,385 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:46,386 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.57799999999952. input_tokens=285, output_tokens=184
19:26:46,425 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:46,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.405999999999949. input_tokens=299, output_tokens=202
19:26:46,451 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:46,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.532000000000153. input_tokens=343, output_tokens=145
19:26:46,708 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:46,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.469000000000051. input_tokens=288, output_tokens=176
19:26:47,425 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:47,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.688000000000102. input_tokens=286, output_tokens=145
19:26:47,669 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:47,669 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.061999999999898. input_tokens=278, output_tokens=165
19:26:47,754 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:47,755 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.546999999999571. input_tokens=312, output_tokens=188
19:26:48,4 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:48,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.030999999999949. input_tokens=304, output_tokens=165
19:26:48,163 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:48,164 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.592999999999847. input_tokens=279, output_tokens=196
19:26:48,333 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:48,333 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2340000000003783. input_tokens=291, output_tokens=189
19:26:48,487 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:48,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.405999999999949. input_tokens=301, output_tokens=186
19:26:48,571 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:48,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.532000000000153. input_tokens=303, output_tokens=200
19:26:48,710 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:48,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8900000000003274. input_tokens=302, output_tokens=214
19:26:48,727 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:48,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.82799999999952. input_tokens=317, output_tokens=204
19:26:49,140 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:49,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.45299999999952. input_tokens=402, output_tokens=239
19:26:49,549 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:49,550 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.67200000000048. input_tokens=457, output_tokens=199
19:26:49,871 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:49,872 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:49,873 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.265000000000327. input_tokens=307, output_tokens=303
19:26:49,873 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4210000000002765. input_tokens=292, output_tokens=174
19:26:49,880 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:49,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.32799999999952. input_tokens=300, output_tokens=237
19:26:50,52 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:50,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.5. input_tokens=283, output_tokens=227
19:26:50,160 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:50,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.733999999999469. input_tokens=294, output_tokens=159
19:26:50,200 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:50,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.844000000000051. input_tokens=364, output_tokens=321
19:26:50,388 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:50,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.094000000000051. input_tokens=347, output_tokens=247
19:26:50,499 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:50,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.140000000000327. input_tokens=319, output_tokens=260
19:26:50,830 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:50,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.344000000000051. input_tokens=326, output_tokens=387
19:26:51,29 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:51,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.594000000000051. input_tokens=300, output_tokens=215
19:26:51,500 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:51,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7340000000003783. input_tokens=284, output_tokens=219
19:26:52,11 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.625. input_tokens=316, output_tokens=309
19:26:52,155 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,155 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.985000000000582. input_tokens=301, output_tokens=246
19:26:52,236 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5150000000003274. input_tokens=279, output_tokens=178
19:26:52,398 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,398 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.688000000000102. input_tokens=293, output_tokens=324
19:26:52,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.532000000000153. input_tokens=280, output_tokens=133
19:26:52,522 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.391000000000531. input_tokens=286, output_tokens=189
19:26:52,587 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.65599999999995. input_tokens=331, output_tokens=461
19:26:52,729 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.063000000000102. input_tokens=305, output_tokens=303
19:26:52,730 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,731 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6719999999995707. input_tokens=308, output_tokens=150
19:26:52,751 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.20299999999952. input_tokens=291, output_tokens=162
19:26:52,772 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,773 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.282000000000153. input_tokens=293, output_tokens=200
19:26:52,885 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:52,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.875. input_tokens=309, output_tokens=269
19:26:53,328 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:53,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4530000000004293. input_tokens=314, output_tokens=181
19:26:53,445 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:53,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.094000000000051. input_tokens=386, output_tokens=454
19:26:53,729 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:53,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.844000000000051. input_tokens=288, output_tokens=213
19:26:54,5 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:54,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.671999999999571. input_tokens=308, output_tokens=194
19:26:54,599 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:54,601 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.890999999999622. input_tokens=377, output_tokens=351
19:26:54,977 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:54,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.813000000000102. input_tokens=303, output_tokens=193
19:26:55,775 httpx INFO HTTP Request: POST http://localhost:3000/v1/chat/completions "HTTP/1.1 200 OK"
19:26:55,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.203000000000429. input_tokens=308, output_tokens=386
19:26:55,812 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
19:26:55,995 graphrag.index.run INFO Running workflow: join_text_units_to_covariate_ids...
19:26:55,995 graphrag.index.run INFO dependencies for join_text_units_to_covariate_ids: ['create_final_covariates']
19:26:55,995 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
19:26:56,13 datashaper.workflow.workflow INFO executing verb select
19:26:56,19 datashaper.workflow.workflow INFO executing verb aggregate_override
19:26:56,24 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_covariate_ids.parquet
19:26:56,222 graphrag.index.run INFO Running workflow: create_base_entity_graph...
19:26:56,222 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
19:26:56,222 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
19:26:56,237 datashaper.workflow.workflow INFO executing verb cluster_graph
19:26:56,389 datashaper.workflow.workflow INFO executing verb select
19:26:56,392 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
19:26:56,589 graphrag.index.run INFO Running workflow: create_final_entities...
19:26:56,589 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
19:26:56,589 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
19:26:56,609 datashaper.workflow.workflow INFO executing verb unpack_graph
19:26:56,657 datashaper.workflow.workflow INFO executing verb rename
19:26:56,665 datashaper.workflow.workflow INFO executing verb select
19:26:56,672 datashaper.workflow.workflow INFO executing verb dedupe
19:26:56,689 datashaper.workflow.workflow INFO executing verb rename
19:26:56,698 datashaper.workflow.workflow INFO executing verb filter
19:26:56,716 datashaper.workflow.workflow INFO executing verb text_split
19:26:56,727 datashaper.workflow.workflow INFO executing verb drop
19:26:56,735 datashaper.workflow.workflow INFO executing verb merge
19:26:56,767 datashaper.workflow.workflow INFO executing verb text_embed
19:26:56,769 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:3000/v1
19:26:56,923 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for embedding-2: TPM=0, RPM=0
19:26:56,923 graphrag.index.llm.load_llm INFO create concurrency limiter for embedding-2: 25
19:26:56,933 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 248 inputs via 248 snippets using 16 batches. max_batch_size=16, max_tokens=8191
19:26:56,963 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:56,966 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':']}
19:26:56,967 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:56,967 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:56,968 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':>']}
19:26:56,969 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':>', ':\\u9ec4\\u98ce\\u5cad', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':', ':', '::\\u6c34\\u5e18\\u6d1e(\\u6c34\\u5e18\\u6d1e)\\u662f\\u65e0\\u72ec\\u5904\\u4e8e\\u8377\\u679c\\u5c71\\u4e2d\\u7684\\u4e00\\u4e2a\\u795e\\u79d8\\u5dee\\u7a9f\\uff0c\\u5b83\\u7531\\u6c34\\u5e18\\u6210\\u4e3a\\u9632\\u58c1\\u3002\\u8fde\\u4e4b\\u672a\\u51fa\\u7684\\u6597\\u732b\\u77f3\\u732b\\u53d1\\u73b0\\u4e86\\u8be5\\u5dee\\u7a9f\\uff0c\\u5e76\\u628a\\u5176\\u5f04\\u6210\\u4e86\\u72ec\\u81ea\\u548c\\u72ec\\u7acb\\u7684\\u5bb6\\u56ed\\u3002\\u5728\\u8c01\\u5e72\\u516c\\u5b50\\u7684\\u6545\\u4e8b\\u4e2d\\uff0c\\u6885\\u5c71\\u516d\\u5144\\u5f1f\\u5c31\\u662f\\u8d1f\\u8d23\\u5165\\u4fb5\\u6c34\\u5e18\\u6d1e\\u7684\\u3002\\u8be5\\u5dee\\u7a9f\\u4e5f\\u6210\\u4e3a\\u4e86\\u72ec\\u5b50\\u4eec\\u7684\\u5bb6\\u5eAD\\u3002\\u6c34\\u5e18\\u6d1e\\u4e0d\\u4ec5\\u662f\\u4e00\\u4e2a\\u5b89\\u5168\\u7684\\u9694\\u75c5\\u5730\\uff0c\\u8fd8\\u662f\\u72ec\\u5b50\\u4eec\\u7684\\u795']}
19:26:56,969 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:56,969 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:56,970 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:56,970 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':>', ':>', ':', ':>', ':', ':', ':', ':', ':', ':', ':', ':']}
19:26:56,971 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':>', ':', ':>', ':>', ':', ':', ':>', ':', ':']}
19:26:56,973 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':>', ':>', ':>', ':>', ':__', ':>', ':>', ':', ':>', ':', ':']}
19:26:56,999 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:56,999 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:26:57,0 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:57,1 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':>', ':>', ':>', ':>', ':>', ':', ':>', ':o', ':', ':', ':', ':oo', ':', ':']}
19:26:57,1 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:57,2 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:57,2 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:57,2 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:57,2 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:57,3 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:57,3 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:57,3 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:57,4 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':', ':', ':', ':', ':>', ':', ':', ':', ':', ':', ':', ':']}
19:26:57,4 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':o', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':']}
19:26:57,5 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:26:57,6 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:26:57,7 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':>', ':', ':', ':', ':>', ':>', ':>', ':>', ':>', ':>', ':>', ':>']}
19:26:57,8 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':']}
19:26:57,8 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:26:57,9 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o|', ':|', ':|', ':|', ':', ':|']}
19:26:58,128 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,128 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,131 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:26:58,132 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:26:58,168 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,169 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':>', ':>', ':', ':>', ':', ':', ':', ':', ':', ':', ':', ':']}
19:26:58,177 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,179 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':']}
19:26:58,328 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,328 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,329 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':>', ':>', ':>', ':>', ':>', ':', ':>', ':o', ':', ':', ':', ':oo', ':', ':']}
19:26:58,331 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':o', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':']}
19:26:58,588 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,589 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,589 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,590 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':>', ':', ':>', ':>', ':', ':', ':>', ':', ':']}
19:26:58,591 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':>', ':\\u9ec4\\u98ce\\u5cad', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':', ':', '::\\u6c34\\u5e18\\u6d1e(\\u6c34\\u5e18\\u6d1e)\\u662f\\u65e0\\u72ec\\u5904\\u4e8e\\u8377\\u679c\\u5c71\\u4e2d\\u7684\\u4e00\\u4e2a\\u795e\\u79d8\\u5dee\\u7a9f\\uff0c\\u5b83\\u7531\\u6c34\\u5e18\\u6210\\u4e3a\\u9632\\u58c1\\u3002\\u8fde\\u4e4b\\u672a\\u51fa\\u7684\\u6597\\u732b\\u77f3\\u732b\\u53d1\\u73b0\\u4e86\\u8be5\\u5dee\\u7a9f\\uff0c\\u5e76\\u628a\\u5176\\u5f04\\u6210\\u4e86\\u72ec\\u81ea\\u548c\\u72ec\\u7acb\\u7684\\u5bb6\\u56ed\\u3002\\u5728\\u8c01\\u5e72\\u516c\\u5b50\\u7684\\u6545\\u4e8b\\u4e2d\\uff0c\\u6885\\u5c71\\u516d\\u5144\\u5f1f\\u5c31\\u662f\\u8d1f\\u8d23\\u5165\\u4fb5\\u6c34\\u5e18\\u6d1e\\u7684\\u3002\\u8be5\\u5dee\\u7a9f\\u4e5f\\u6210\\u4e3a\\u4e86\\u72ec\\u5b50\\u4eec\\u7684\\u5bb6\\u5eAD\\u3002\\u6c34\\u5e18\\u6d1e\\u4e0d\\u4ec5\\u662f\\u4e00\\u4e2a\\u5b89\\u5168\\u7684\\u9694\\u75c5\\u5730\\uff0c\\u8fd8\\u662f\\u72ec\\u5b50\\u4eec\\u7684\\u795']}
19:26:58,592 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:26:58,667 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,668 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,669 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':', ':', ':', ':', ':>', ':', ':', ':', ':', ':', ':', ':']}
19:26:58,670 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':>', ':>', ':>', ':>', ':__', ':>', ':>', ':', ':>', ':', ':']}
19:26:58,828 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,828 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,830 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':>']}
19:26:58,832 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':']}
19:26:58,878 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,880 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o|', ':|', ':|', ':|', ':', ':|']}
19:26:58,927 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:58,929 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:26:59,67 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:26:59,69 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':>', ':', ':', ':', ':>', ':>', ':>', ':>', ':>', ':>', ':>', ':>']}
19:27:00,268 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:00,269 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:00,498 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:00,499 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':>', ':>', ':', ':>', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:00,547 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:00,548 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':o', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':']}
19:27:00,687 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:00,689 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':']}
19:27:00,718 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:00,719 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:00,796 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:00,798 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':>', ':>', ':>', ':>', ':>', ':', ':>', ':o', ':', ':', ':', ':oo', ':', ':']}
19:27:00,848 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:00,849 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:00,938 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:00,939 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':>']}
19:27:01,167 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:01,167 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':>', ':', ':>', ':>', ':', ':', ':>', ':', ':']}
19:27:01,218 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:01,219 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':', ':', ':', ':', ':>', ':', ':', ':', ':', ':', ':', ':']}
19:27:01,452 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:01,453 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':>', ':>', ':>', ':>', ':__', ':>', ':>', ':', ':>', ':', ':']}
19:27:01,607 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:01,608 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:01,609 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':>', ':\\u9ec4\\u98ce\\u5cad', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':', ':', '::\\u6c34\\u5e18\\u6d1e(\\u6c34\\u5e18\\u6d1e)\\u662f\\u65e0\\u72ec\\u5904\\u4e8e\\u8377\\u679c\\u5c71\\u4e2d\\u7684\\u4e00\\u4e2a\\u795e\\u79d8\\u5dee\\u7a9f\\uff0c\\u5b83\\u7531\\u6c34\\u5e18\\u6210\\u4e3a\\u9632\\u58c1\\u3002\\u8fde\\u4e4b\\u672a\\u51fa\\u7684\\u6597\\u732b\\u77f3\\u732b\\u53d1\\u73b0\\u4e86\\u8be5\\u5dee\\u7a9f\\uff0c\\u5e76\\u628a\\u5176\\u5f04\\u6210\\u4e86\\u72ec\\u81ea\\u548c\\u72ec\\u7acb\\u7684\\u5bb6\\u56ed\\u3002\\u5728\\u8c01\\u5e72\\u516c\\u5b50\\u7684\\u6545\\u4e8b\\u4e2d\\uff0c\\u6885\\u5c71\\u516d\\u5144\\u5f1f\\u5c31\\u662f\\u8d1f\\u8d23\\u5165\\u4fb5\\u6c34\\u5e18\\u6d1e\\u7684\\u3002\\u8be5\\u5dee\\u7a9f\\u4e5f\\u6210\\u4e3a\\u4e86\\u72ec\\u5b50\\u4eec\\u7684\\u5bb6\\u5eAD\\u3002\\u6c34\\u5e18\\u6d1e\\u4e0d\\u4ec5\\u662f\\u4e00\\u4e2a\\u5b89\\u5168\\u7684\\u9694\\u75c5\\u5730\\uff0c\\u8fd8\\u662f\\u72ec\\u5b50\\u4eec\\u7684\\u795']}
19:27:01,610 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':>', ':', ':', ':', ':>', ':>', ':>', ':>', ':>', ':>', ':>', ':>']}
19:27:01,698 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:01,698 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:01,699 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':']}
19:27:01,700 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o|', ':|', ':|', ':|', ':', ':|']}
19:27:01,897 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:01,899 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:04,968 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:04,968 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:04,969 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':>', ':>', ':>', ':>', ':>', ':', ':>', ':o', ':', ':', ':', ':oo', ':', ':']}
19:27:04,970 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':o', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':']}
19:27:05,157 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:05,158 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:05,197 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:05,199 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:05,297 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:05,298 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':>', ':>', ':', ':>', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:05,427 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:05,428 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':']}
19:27:05,607 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:05,608 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:05,707 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:05,708 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':>']}
19:27:05,957 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:05,958 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':', ':', ':', ':', ':>', ':', ':', ':', ':', ':', ':', ':']}
19:27:06,78 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:06,79 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':>', ':', ':', ':', ':>', ':>', ':>', ':>', ':>', ':>', ':>', ':>']}
19:27:06,158 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:06,158 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':>', ':', ':>', ':>', ':', ':', ':>', ':', ':']}
19:27:06,317 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:06,319 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':>', ':>', ':>', ':>', ':__', ':>', ':>', ':', ':>', ':', ':']}
19:27:06,367 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:06,368 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o|', ':|', ':|', ':|', ':', ':|']}
19:27:06,414 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:06,415 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':>', ':\\u9ec4\\u98ce\\u5cad', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':', ':', '::\\u6c34\\u5e18\\u6d1e(\\u6c34\\u5e18\\u6d1e)\\u662f\\u65e0\\u72ec\\u5904\\u4e8e\\u8377\\u679c\\u5c71\\u4e2d\\u7684\\u4e00\\u4e2a\\u795e\\u79d8\\u5dee\\u7a9f\\uff0c\\u5b83\\u7531\\u6c34\\u5e18\\u6210\\u4e3a\\u9632\\u58c1\\u3002\\u8fde\\u4e4b\\u672a\\u51fa\\u7684\\u6597\\u732b\\u77f3\\u732b\\u53d1\\u73b0\\u4e86\\u8be5\\u5dee\\u7a9f\\uff0c\\u5e76\\u628a\\u5176\\u5f04\\u6210\\u4e86\\u72ec\\u81ea\\u548c\\u72ec\\u7acb\\u7684\\u5bb6\\u56ed\\u3002\\u5728\\u8c01\\u5e72\\u516c\\u5b50\\u7684\\u6545\\u4e8b\\u4e2d\\uff0c\\u6885\\u5c71\\u516d\\u5144\\u5f1f\\u5c31\\u662f\\u8d1f\\u8d23\\u5165\\u4fb5\\u6c34\\u5e18\\u6d1e\\u7684\\u3002\\u8be5\\u5dee\\u7a9f\\u4e5f\\u6210\\u4e3a\\u4e86\\u72ec\\u5b50\\u4eec\\u7684\\u5bb6\\u5eAD\\u3002\\u6c34\\u5e18\\u6d1e\\u4e0d\\u4ec5\\u662f\\u4e00\\u4e2a\\u5b89\\u5168\\u7684\\u9694\\u75c5\\u5730\\uff0c\\u8fd8\\u662f\\u72ec\\u5b50\\u4eec\\u7684\\u795']}
19:27:06,457 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:06,457 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:06,459 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:06,460 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':']}
19:27:13,200 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:13,202 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':o', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':']}
19:27:13,427 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:13,428 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':>', ':>', ':>', ':>', ':>', ':', ':>', ':o', ':', ':', ':', ':oo', ':', ':']}
19:27:13,737 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:13,738 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:13,835 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:13,836 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:13,887 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:13,888 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':>', ':>', ':', ':>', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:14,97 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:14,97 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:14,98 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:14,99 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':>']}
19:27:14,217 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:14,218 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':>', ':', ':', ':', ':>', ':>', ':>', ':>', ':>', ':>', ':>', ':>']}
19:27:14,387 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:14,388 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':>', ':>', ':>', ':>', ':__', ':>', ':>', ':', ':>', ':', ':']}
19:27:14,497 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:14,498 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':']}
19:27:14,767 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:14,768 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':']}
19:27:14,797 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:14,798 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':>', ':\\u9ec4\\u98ce\\u5cad', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':', ':', '::\\u6c34\\u5e18\\u6d1e(\\u6c34\\u5e18\\u6d1e)\\u662f\\u65e0\\u72ec\\u5904\\u4e8e\\u8377\\u679c\\u5c71\\u4e2d\\u7684\\u4e00\\u4e2a\\u795e\\u79d8\\u5dee\\u7a9f\\uff0c\\u5b83\\u7531\\u6c34\\u5e18\\u6210\\u4e3a\\u9632\\u58c1\\u3002\\u8fde\\u4e4b\\u672a\\u51fa\\u7684\\u6597\\u732b\\u77f3\\u732b\\u53d1\\u73b0\\u4e86\\u8be5\\u5dee\\u7a9f\\uff0c\\u5e76\\u628a\\u5176\\u5f04\\u6210\\u4e86\\u72ec\\u81ea\\u548c\\u72ec\\u7acb\\u7684\\u5bb6\\u56ed\\u3002\\u5728\\u8c01\\u5e72\\u516c\\u5b50\\u7684\\u6545\\u4e8b\\u4e2d\\uff0c\\u6885\\u5c71\\u516d\\u5144\\u5f1f\\u5c31\\u662f\\u8d1f\\u8d23\\u5165\\u4fb5\\u6c34\\u5e18\\u6d1e\\u7684\\u3002\\u8be5\\u5dee\\u7a9f\\u4e5f\\u6210\\u4e3a\\u4e86\\u72ec\\u5b50\\u4eec\\u7684\\u5bb6\\u5eAD\\u3002\\u6c34\\u5e18\\u6d1e\\u4e0d\\u4ec5\\u662f\\u4e00\\u4e2a\\u5b89\\u5168\\u7684\\u9694\\u75c5\\u5730\\uff0c\\u8fd8\\u662f\\u72ec\\u5b50\\u4eec\\u7684\\u795']}
19:27:14,967 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:14,968 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':', ':', ':', ':', ':>', ':', ':', ':', ':', ':', ':', ':']}
19:27:15,127 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:15,129 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':>', ':', ':>', ':>', ':', ':', ':>', ':', ':']}
19:27:15,177 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:15,178 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:15,417 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:15,418 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o|', ':|', ':|', ':|', ':', ':|']}
19:27:23,212 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:23,213 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':o', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':']}
19:27:23,487 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:23,489 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':>', ':>', ':>', ':>', ':>', ':', ':>', ':o', ':', ':', ':', ':oo', ':', ':']}
19:27:23,797 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:23,798 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:23,877 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:23,878 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:23,882 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:23,883 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':>', ':>', ':', ':>', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:24,176 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:24,178 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':>']}
19:27:24,178 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:24,179 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:24,267 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:24,268 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':>', ':', ':', ':', ':>', ':>', ':>', ':>', ':>', ':>', ':>', ':>']}
19:27:24,437 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:24,439 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':>', ':>', ':>', ':>', ':__', ':>', ':>', ':', ':>', ':', ':']}
19:27:24,546 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:24,547 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':']}
19:27:24,828 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:24,829 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':']}
19:27:24,877 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:24,878 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':>', ':\\u9ec4\\u98ce\\u5cad', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':', ':', '::\\u6c34\\u5e18\\u6d1e(\\u6c34\\u5e18\\u6d1e)\\u662f\\u65e0\\u72ec\\u5904\\u4e8e\\u8377\\u679c\\u5c71\\u4e2d\\u7684\\u4e00\\u4e2a\\u795e\\u79d8\\u5dee\\u7a9f\\uff0c\\u5b83\\u7531\\u6c34\\u5e18\\u6210\\u4e3a\\u9632\\u58c1\\u3002\\u8fde\\u4e4b\\u672a\\u51fa\\u7684\\u6597\\u732b\\u77f3\\u732b\\u53d1\\u73b0\\u4e86\\u8be5\\u5dee\\u7a9f\\uff0c\\u5e76\\u628a\\u5176\\u5f04\\u6210\\u4e86\\u72ec\\u81ea\\u548c\\u72ec\\u7acb\\u7684\\u5bb6\\u56ed\\u3002\\u5728\\u8c01\\u5e72\\u516c\\u5b50\\u7684\\u6545\\u4e8b\\u4e2d\\uff0c\\u6885\\u5c71\\u516d\\u5144\\u5f1f\\u5c31\\u662f\\u8d1f\\u8d23\\u5165\\u4fb5\\u6c34\\u5e18\\u6d1e\\u7684\\u3002\\u8be5\\u5dee\\u7a9f\\u4e5f\\u6210\\u4e3a\\u4e86\\u72ec\\u5b50\\u4eec\\u7684\\u5bb6\\u5eAD\\u3002\\u6c34\\u5e18\\u6d1e\\u4e0d\\u4ec5\\u662f\\u4e00\\u4e2a\\u5b89\\u5168\\u7684\\u9694\\u75c5\\u5730\\uff0c\\u8fd8\\u662f\\u72ec\\u5b50\\u4eec\\u7684\\u795']}
19:27:25,16 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:25,18 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':', ':', ':', ':', ':>', ':', ':', ':', ':', ':', ':', ':']}
19:27:25,187 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:25,189 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':>', ':', ':>', ':>', ':', ':', ':>', ':', ':']}
19:27:25,236 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:25,238 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:25,467 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:25,468 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o|', ':|', ':|', ':|', ':', ':|']}
19:27:33,223 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:33,224 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':o', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':']}
19:27:33,546 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:33,547 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':>', ':>', ':>', ':>', ':>', ':', ':>', ':o', ':', ':', ':', ':oo', ':', ':']}
19:27:33,876 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:33,877 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:33,883 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:33,884 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':>', ':>', ':', ':>', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:33,926 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:33,927 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:34,236 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:34,237 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:34,238 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':>']}
19:27:34,239 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:34,326 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:34,327 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':>', ':', ':', ':', ':>', ':>', ':>', ':>', ':>', ':>', ':>', ':>']}
19:27:34,506 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:34,507 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':>', ':>', ':>', ':>', ':__', ':>', ':>', ':', ':>', ':', ':']}
19:27:34,596 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:34,598 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':']}
19:27:34,876 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:34,877 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':']}
19:27:34,926 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:34,928 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':>', ':\\u9ec4\\u98ce\\u5cad', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':', ':', '::\\u6c34\\u5e18\\u6d1e(\\u6c34\\u5e18\\u6d1e)\\u662f\\u65e0\\u72ec\\u5904\\u4e8e\\u8377\\u679c\\u5c71\\u4e2d\\u7684\\u4e00\\u4e2a\\u795e\\u79d8\\u5dee\\u7a9f\\uff0c\\u5b83\\u7531\\u6c34\\u5e18\\u6210\\u4e3a\\u9632\\u58c1\\u3002\\u8fde\\u4e4b\\u672a\\u51fa\\u7684\\u6597\\u732b\\u77f3\\u732b\\u53d1\\u73b0\\u4e86\\u8be5\\u5dee\\u7a9f\\uff0c\\u5e76\\u628a\\u5176\\u5f04\\u6210\\u4e86\\u72ec\\u81ea\\u548c\\u72ec\\u7acb\\u7684\\u5bb6\\u56ed\\u3002\\u5728\\u8c01\\u5e72\\u516c\\u5b50\\u7684\\u6545\\u4e8b\\u4e2d\\uff0c\\u6885\\u5c71\\u516d\\u5144\\u5f1f\\u5c31\\u662f\\u8d1f\\u8d23\\u5165\\u4fb5\\u6c34\\u5e18\\u6d1e\\u7684\\u3002\\u8be5\\u5dee\\u7a9f\\u4e5f\\u6210\\u4e3a\\u4e86\\u72ec\\u5b50\\u4eec\\u7684\\u5bb6\\u5eAD\\u3002\\u6c34\\u5e18\\u6d1e\\u4e0d\\u4ec5\\u662f\\u4e00\\u4e2a\\u5b89\\u5168\\u7684\\u9694\\u75c5\\u5730\\uff0c\\u8fd8\\u662f\\u72ec\\u5b50\\u4eec\\u7684\\u795']}
19:27:35,86 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:35,88 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':', ':', ':', ':', ':>', ':', ':', ':', ':', ':', ':', ':']}
19:27:35,256 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:35,257 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':>', ':', ':>', ':>', ':', ':', ':>', ':', ':']}
19:27:35,306 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:35,307 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:35,517 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:35,518 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o|', ':|', ':|', ':|', ':', ':|']}
19:27:43,234 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:43,236 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':o', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':']}
19:27:43,586 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:43,588 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':>', ':>', ':>', ':>', ':>', ':', ':>', ':o', ':', ':', ':', ':oo', ':', ':']}
19:27:43,915 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:43,916 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:43,946 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:43,947 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':>', ':>', ':', ':>', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:43,995 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:43,997 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:44,306 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:44,307 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:44,316 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:44,318 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':>']}
19:27:44,367 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:44,368 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':>', ':', ':', ':', ':>', ':>', ':>', ':>', ':>', ':>', ':>', ':>']}
19:27:44,566 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:44,566 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':>', ':>', ':>', ':>', ':__', ':>', ':>', ':', ':>', ':', ':']}
19:27:44,666 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:44,667 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':']}
19:27:44,936 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:44,938 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':']}
19:27:44,986 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:44,988 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':>', ':\\u9ec4\\u98ce\\u5cad', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':', ':', '::\\u6c34\\u5e18\\u6d1e(\\u6c34\\u5e18\\u6d1e)\\u662f\\u65e0\\u72ec\\u5904\\u4e8e\\u8377\\u679c\\u5c71\\u4e2d\\u7684\\u4e00\\u4e2a\\u795e\\u79d8\\u5dee\\u7a9f\\uff0c\\u5b83\\u7531\\u6c34\\u5e18\\u6210\\u4e3a\\u9632\\u58c1\\u3002\\u8fde\\u4e4b\\u672a\\u51fa\\u7684\\u6597\\u732b\\u77f3\\u732b\\u53d1\\u73b0\\u4e86\\u8be5\\u5dee\\u7a9f\\uff0c\\u5e76\\u628a\\u5176\\u5f04\\u6210\\u4e86\\u72ec\\u81ea\\u548c\\u72ec\\u7acb\\u7684\\u5bb6\\u56ed\\u3002\\u5728\\u8c01\\u5e72\\u516c\\u5b50\\u7684\\u6545\\u4e8b\\u4e2d\\uff0c\\u6885\\u5c71\\u516d\\u5144\\u5f1f\\u5c31\\u662f\\u8d1f\\u8d23\\u5165\\u4fb5\\u6c34\\u5e18\\u6d1e\\u7684\\u3002\\u8be5\\u5dee\\u7a9f\\u4e5f\\u6210\\u4e3a\\u4e86\\u72ec\\u5b50\\u4eec\\u7684\\u5bb6\\u5eAD\\u3002\\u6c34\\u5e18\\u6d1e\\u4e0d\\u4ec5\\u662f\\u4e00\\u4e2a\\u5b89\\u5168\\u7684\\u9694\\u75c5\\u5730\\uff0c\\u8fd8\\u662f\\u72ec\\u5b50\\u4eec\\u7684\\u795']}
19:27:45,136 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:45,136 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':', ':', ':', ':', ':>', ':', ':', ':', ':', ':', ':', ':']}
19:27:45,316 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:45,318 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':>', ':', ':>', ':>', ':', ':', ':>', ':', ':']}
19:27:45,365 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:45,367 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:45,576 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:45,577 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o|', ':|', ':|', ':|', ':', ':|']}
19:27:53,264 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:53,265 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':o', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':']}
19:27:53,646 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:53,647 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':>', ':>', ':>', ':>', ':>', ':', ':>', ':o', ':', ':', ':', ':oo', ':', ':']}
19:27:53,965 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:53,967 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:53,967 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:53,968 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':>', ':>', ':', ':>', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:54,55 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:54,57 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:54,366 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:54,368 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':>']}
19:27:54,368 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:54,369 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':>', ':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:54,380 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:54,382 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':>', ':', ':', ':', ':>', ':>', ':>', ':>', ':>', ':>', ':>', ':>']}
19:27:54,636 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:54,637 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':>', ':>', ':>', ':>', ':__', ':>', ':>', ':', ':>', ':', ':']}
19:27:54,726 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:54,728 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':']}
19:27:55,6 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:55,7 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':']}
19:27:55,56 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:55,57 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':>', ':', ':>', ':\\u9ec4\\u98ce\\u5cad', ':', ':', ':', ':', ':>', ':>', ':>', ':', ':', ':', ':', '::\\u6c34\\u5e18\\u6d1e(\\u6c34\\u5e18\\u6d1e)\\u662f\\u65e0\\u72ec\\u5904\\u4e8e\\u8377\\u679c\\u5c71\\u4e2d\\u7684\\u4e00\\u4e2a\\u795e\\u79d8\\u5dee\\u7a9f\\uff0c\\u5b83\\u7531\\u6c34\\u5e18\\u6210\\u4e3a\\u9632\\u58c1\\u3002\\u8fde\\u4e4b\\u672a\\u51fa\\u7684\\u6597\\u732b\\u77f3\\u732b\\u53d1\\u73b0\\u4e86\\u8be5\\u5dee\\u7a9f\\uff0c\\u5e76\\u628a\\u5176\\u5f04\\u6210\\u4e86\\u72ec\\u81ea\\u548c\\u72ec\\u7acb\\u7684\\u5bb6\\u56ed\\u3002\\u5728\\u8c01\\u5e72\\u516c\\u5b50\\u7684\\u6545\\u4e8b\\u4e2d\\uff0c\\u6885\\u5c71\\u516d\\u5144\\u5f1f\\u5c31\\u662f\\u8d1f\\u8d23\\u5165\\u4fb5\\u6c34\\u5e18\\u6d1e\\u7684\\u3002\\u8be5\\u5dee\\u7a9f\\u4e5f\\u6210\\u4e3a\\u4e86\\u72ec\\u5b50\\u4eec\\u7684\\u5bb6\\u5eAD\\u3002\\u6c34\\u5e18\\u6d1e\\u4e0d\\u4ec5\\u662f\\u4e00\\u4e2a\\u5b89\\u5168\\u7684\\u9694\\u75c5\\u5730\\uff0c\\u8fd8\\u662f\\u72ec\\u5b50\\u4eec\\u7684\\u795']}
19:27:55,196 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:55,196 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':>', ':>', ':>', ':', ':', ':', ':', ':>', ':', ':', ':', ':', ':', ':', ':']}
19:27:55,376 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:55,378 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':>', ':', ':>', ':>', ':', ':', ':>', ':', ':']}
19:27:55,425 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:55,426 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':', ':']}
19:27:55,626 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:27:55,627 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o|', ':|', ':|', ':|', ':', ':|']}
19:28:03,278 httpx INFO HTTP Request: POST http://localhost:3000/v1/embeddings "HTTP/1.1 500 Internal Server Error"
19:28:03,279 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': [':', ':', ':o', ':', ':', ':o', ':', ':', ':o', ':', ':', ':', ':', ':', ':', ':']}
19:28:03,279 datashaper.workflow.workflow ERROR Error executing verb "text_embed" in create_final_entities: Error code: 500 - {'error': {'message': 'invalid input length, zhipu only support one input (request id: 2024120519280327806959597280351)', 'type': 'one_api_error', 'param': '', 'code': 'convert_request_failed'}}
Traceback (most recent call last):
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\datashaper\workflow\workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\index\verbs\text\embed\text_embed.py", line 105, in text_embed
    return await _text_embed_in_memory(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\index\verbs\text\embed\text_embed.py", line 130, in _text_embed_in_memory
    result = await strategy_exec(texts, callbacks, cache, strategy_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 62, in run
    embeddings = await _execute(llm, text_batches, ticker, semaphore)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 106, in _execute
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\asyncio\tasks.py", line 349, in __wakeup
    future.result()
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\asyncio\tasks.py", line 277, in __step
    result = coro.send(None)
             ^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 100, in embed
    chunk_embeddings = await llm(chunk)
                       ^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\openai\openai_embeddings_llm.py", line 36, in _execute_llm
    embedding = await self.client.embeddings.create(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\openai\resources\embeddings.py", line 236, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\openai\_base_client.py", line 1843, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\openai\_base_client.py", line 1537, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\openai\_base_client.py", line 1638, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.InternalServerError: Error code: 500 - {'error': {'message': 'invalid input length, zhipu only support one input (request id: 2024120519280327806959597280351)', 'type': 'one_api_error', 'param': '', 'code': 'convert_request_failed'}}
19:28:03,284 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "text_embed" in create_final_entities: Error code: 500 - {'error': {'message': 'invalid input length, zhipu only support one input (request id: 2024120519280327806959597280351)', 'type': 'one_api_error', 'param': '', 'code': 'convert_request_failed'}} details=None
19:28:03,294 graphrag.index.run ERROR error running workflow create_final_entities
Traceback (most recent call last):
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\index\run.py", line 325, in run_pipeline
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\datashaper\workflow\workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\datashaper\workflow\workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\index\verbs\text\embed\text_embed.py", line 105, in text_embed
    return await _text_embed_in_memory(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\index\verbs\text\embed\text_embed.py", line 130, in _text_embed_in_memory
    result = await strategy_exec(texts, callbacks, cache, strategy_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 62, in run
    embeddings = await _execute(llm, text_batches, ticker, semaphore)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 106, in _execute
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\asyncio\tasks.py", line 349, in __wakeup
    future.result()
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\asyncio\tasks.py", line 277, in __step
    result = coro.send(None)
             ^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 100, in embed
    chunk_embeddings = await llm(chunk)
                       ^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\tenacity\__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\tenacity\__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\base\base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\graphrag\llm\openai\openai_embeddings_llm.py", line 36, in _execute_llm
    embedding = await self.client.embeddings.create(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\openai\resources\embeddings.py", line 236, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\openai\_base_client.py", line 1843, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\openai\_base_client.py", line 1537, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\CodeSoftware\Anaconda\envs\GraphRAG\Lib\site-packages\openai\_base_client.py", line 1638, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.InternalServerError: Error code: 500 - {'error': {'message': 'invalid input length, zhipu only support one input (request id: 2024120519280327806959597280351)', 'type': 'one_api_error', 'param': '', 'code': 'convert_request_failed'}}
19:28:03,297 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
